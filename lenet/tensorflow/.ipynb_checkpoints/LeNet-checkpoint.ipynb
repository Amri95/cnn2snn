{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.03125\n",
      "time:  0.0300660133362\n",
      "step 100, training accuracy 0.90625\n",
      "time:  0.500409841537\n",
      "step 200, training accuracy 0.90625\n",
      "time:  0.402717113495\n",
      "step 300, training accuracy 0.953125\n",
      "time:  0.427516937256\n",
      "step 400, training accuracy 0.9375\n",
      "time:  0.390737056732\n",
      "step 500, training accuracy 0.890625\n",
      "time:  0.398172855377\n",
      "step 600, training accuracy 0.96875\n",
      "time:  0.390864133835\n",
      "step 700, training accuracy 0.90625\n",
      "time:  0.391039848328\n",
      "step 800, training accuracy 0.921875\n",
      "time:  0.403053998947\n",
      "step 900, training accuracy 0.9375\n",
      "time:  0.448539018631\n",
      "step 1000, training accuracy 0.96875\n",
      "time:  0.495213985443\n",
      "step 1100, training accuracy 1\n",
      "time:  0.390115022659\n",
      "step 1200, training accuracy 0.9375\n",
      "time:  0.479073047638\n",
      "step 1300, training accuracy 1\n",
      "time:  0.397478103638\n",
      "step 1400, training accuracy 0.96875\n",
      "time:  0.399816036224\n",
      "step 1500, training accuracy 0.984375\n",
      "time:  0.462018966675\n",
      "step 1600, training accuracy 0.984375\n",
      "time:  0.415992021561\n",
      "step 1700, training accuracy 0.96875\n",
      "time:  0.397966861725\n",
      "step 1800, training accuracy 0.984375\n",
      "time:  0.400823116302\n",
      "step 1900, training accuracy 0.984375\n",
      "time:  0.421434879303\n",
      "step 2000, training accuracy 0.953125\n",
      "time:  0.416280031204\n",
      "step 2100, training accuracy 0.96875\n",
      "time:  0.388611078262\n",
      "step 2200, training accuracy 0.96875\n",
      "time:  0.464761018753\n",
      "step 2300, training accuracy 0.984375\n",
      "time:  0.407627820969\n",
      "step 2400, training accuracy 0.984375\n",
      "time:  0.418949127197\n",
      "step 2500, training accuracy 1\n",
      "time:  0.438104867935\n",
      "step 2600, training accuracy 0.9375\n",
      "time:  0.402049064636\n",
      "step 2700, training accuracy 0.96875\n",
      "time:  0.418972015381\n",
      "step 2800, training accuracy 0.96875\n",
      "time:  0.471919059753\n",
      "step 2900, training accuracy 0.984375\n",
      "time:  0.445252895355\n",
      "step 3000, training accuracy 1\n",
      "time:  0.390628099442\n",
      "step 3100, training accuracy 0.96875\n",
      "time:  0.396632909775\n",
      "step 3200, training accuracy 1\n",
      "time:  0.398416996002\n",
      "step 3300, training accuracy 1\n",
      "time:  0.413503170013\n",
      "step 3400, training accuracy 1\n",
      "time:  0.399772882462\n",
      "step 3500, training accuracy 0.96875\n",
      "time:  0.395972967148\n",
      "step 3600, training accuracy 0.9375\n",
      "time:  0.399076938629\n",
      "step 3700, training accuracy 1\n",
      "time:  0.386973142624\n",
      "step 3800, training accuracy 0.984375\n",
      "time:  0.412307024002\n",
      "step 3900, training accuracy 0.96875\n",
      "time:  0.474941015244\n",
      "step 4000, training accuracy 0.984375\n",
      "time:  0.441421985626\n",
      "step 4100, training accuracy 0.953125\n",
      "time:  0.389456033707\n",
      "step 4200, training accuracy 1\n",
      "time:  0.412170886993\n",
      "step 4300, training accuracy 0.984375\n",
      "time:  0.415280103683\n",
      "step 4400, training accuracy 0.984375\n",
      "time:  0.395484924316\n",
      "step 4500, training accuracy 0.96875\n",
      "time:  0.435034036636\n",
      "step 4600, training accuracy 1\n",
      "time:  0.429780006409\n",
      "step 4700, training accuracy 1\n",
      "time:  0.431596040726\n",
      "step 4800, training accuracy 0.984375\n",
      "time:  0.439503908157\n",
      "step 4900, training accuracy 0.984375\n",
      "time:  0.415575027466\n",
      "step 5000, training accuracy 0.984375\n",
      "time:  0.432839870453\n",
      "step 5100, training accuracy 0.984375\n",
      "time:  0.389711141586\n",
      "step 5200, training accuracy 0.984375\n",
      "time:  0.399951934814\n",
      "step 5300, training accuracy 0.984375\n",
      "time:  0.389761924744\n",
      "step 5400, training accuracy 0.984375\n",
      "time:  0.403424024582\n",
      "step 5500, training accuracy 0.953125\n",
      "time:  0.476445198059\n",
      "step 5600, training accuracy 0.984375\n",
      "time:  0.40434384346\n",
      "step 5700, training accuracy 0.953125\n",
      "time:  0.424055099487\n",
      "step 5800, training accuracy 0.984375\n",
      "time:  0.418647050858\n",
      "step 5900, training accuracy 0.96875\n",
      "time:  0.39636182785\n",
      "step 6000, training accuracy 0.984375\n",
      "time:  0.405342102051\n",
      "step 6100, training accuracy 0.953125\n",
      "time:  0.409265995026\n",
      "step 6200, training accuracy 0.96875\n",
      "time:  0.405879020691\n",
      "step 6300, training accuracy 1\n",
      "time:  0.415444850922\n",
      "step 6400, training accuracy 0.984375\n",
      "time:  0.408523082733\n",
      "step 6500, training accuracy 0.984375\n",
      "time:  0.415118932724\n",
      "step 6600, training accuracy 0.953125\n",
      "time:  0.4472219944\n",
      "step 6700, training accuracy 1\n",
      "time:  0.409273147583\n",
      "step 6800, training accuracy 0.984375\n",
      "time:  0.3889939785\n",
      "step 6900, training accuracy 0.984375\n",
      "time:  0.398958921432\n",
      "step 7000, training accuracy 0.953125\n",
      "time:  0.432011127472\n",
      "step 7100, training accuracy 1\n",
      "time:  0.392659902573\n",
      "step 7200, training accuracy 0.984375\n",
      "time:  0.402766942978\n",
      "step 7300, training accuracy 1\n",
      "time:  0.38786315918\n",
      "step 7400, training accuracy 0.984375\n",
      "time:  0.418495893478\n",
      "step 7500, training accuracy 1\n",
      "time:  0.443610906601\n",
      "step 7600, training accuracy 0.96875\n",
      "time:  0.38796210289\n",
      "step 7700, training accuracy 1\n",
      "time:  0.392831087112\n",
      "step 7800, training accuracy 1\n",
      "time:  0.399238824844\n",
      "step 7900, training accuracy 0.96875\n",
      "time:  0.451533079147\n",
      "step 8000, training accuracy 1\n",
      "time:  0.457577943802\n",
      "step 8100, training accuracy 0.96875\n",
      "time:  0.394405126572\n",
      "step 8200, training accuracy 1\n",
      "time:  0.406213998795\n",
      "step 8300, training accuracy 0.984375\n",
      "time:  0.396325826645\n",
      "step 8400, training accuracy 0.96875\n",
      "time:  0.423741102219\n",
      "step 8500, training accuracy 1\n",
      "time:  0.437681913376\n",
      "step 8600, training accuracy 1\n",
      "time:  0.407557010651\n",
      "step 8700, training accuracy 1\n",
      "time:  0.438361167908\n",
      "step 8800, training accuracy 1\n",
      "time:  0.435217857361\n",
      "step 8900, training accuracy 0.984375\n",
      "time:  0.456149101257\n",
      "step 9000, training accuracy 1\n",
      "time:  0.414974927902\n",
      "step 9100, training accuracy 0.984375\n",
      "time:  0.392883062363\n",
      "step 9200, training accuracy 1\n",
      "time:  0.389623880386\n",
      "step 9300, training accuracy 0.96875\n",
      "time:  0.399308204651\n",
      "step 9400, training accuracy 0.953125\n",
      "time:  0.422851800919\n",
      "step 9500, training accuracy 1\n",
      "time:  0.403450965881\n",
      "step 9600, training accuracy 0.984375\n",
      "time:  0.392506122589\n",
      "step 9700, training accuracy 0.984375\n",
      "time:  0.396900892258\n",
      "step 9800, training accuracy 1\n",
      "time:  0.398350000381\n",
      "step 9900, training accuracy 1\n",
      "time:  0.409312009811\n",
      "Model saved in file:  output/model/model.ckpt\n",
      "test accuracy 0.9888\n",
      "[[ 0.06758393 -0.07989222  0.05684345  0.01389611 -0.06906519]\n",
      " [-0.10053111  0.02082201  0.03176677  0.05098581  0.10551897]\n",
      " [-0.04763233  0.14485142  0.13259497  0.03474766  0.0300314 ]\n",
      " [ 0.09289741  0.12133099  0.08704627  0.16797884  0.21935369]\n",
      " [ 0.08943743  0.03837997  0.06474397 -0.08109991 -0.16843852]]\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  18  20  17   7   3   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0  19  38  61  59  43  30   8   0   0   0   0   0   0  13\n",
      "   31  27  16   5   0   0]\n",
      " [  0   0  39  80 123 182 202 175 146 132 120 109 112 111 124 121 112 105\n",
      "  101  84  48  16   0   0]\n",
      " [  0   0  28  73 112 168 180 178 184 189 199 205 221 223 234 201 177 181\n",
      "  156 124  64  18   0   0]\n",
      " [  0   0  19  33  58  81  60  69  92  98 114 126 131 142 118 119 145 164\n",
      "  155 117  47   0   0   0]\n",
      " [  0   0   0   0  13   2  16  29  40  41  44  44  41  51  20  62 129 152\n",
      "  170  99  18   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   4   3   6   0   0  66 132 186\n",
      "  177  68   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   0   0   0   0  42 131 179 204\n",
      "  116  16   0   5   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  16  83 138 181 170\n",
      "   58   0   2   1   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  38 117 168 189  98\n",
      "    2   0   2   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0  76 138 173 154  48\n",
      "    0   7   2   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0  41 120 166 193 104   0\n",
      "    0   4   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   9  98 155 197 169  45   0\n",
      "    6   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0  71 146 194 200  97   0   0\n",
      "    4   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0  41 132 183 205 143  22   0   6\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  13 108 164 197 168  54   0   0   4\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0  67 122 184 191  87   0   0   8   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0  37 101 170 211 146  49   0   6   1   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  13  84 156 210 212 105  22   4   4   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  12  51 102 211 243 168  78   1   3   0   0   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0  38 104 153 182 153  94  23   0   0   0   0   0\n",
      "    0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-  \n",
    "\n",
    "from sys import path\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import extract_mnist\n",
    "import scipy.io as sio\n",
    "\n",
    "# Parameter\n",
    "batch_size = 64\n",
    "isTrain = True\n",
    "\n",
    "#初始化单个卷积核上的参数\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#输入特征x，用卷积核W进行卷积运算，strides为卷积核移动步长，\n",
    "#padding表示是否需要补齐边缘像素使输出图像大小不变\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "#对x进行最大池化操作，ksize进行池化的范围，\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# average pooling\n",
    "def avg_pool_2x2(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1],strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "def input_poisson(x_image, level):\n",
    "    #print x_image\n",
    "    x_image = (x_image+0.5) * level\n",
    "    \n",
    "    w = x_image.shape[0]\n",
    "    h = x_image.shape[1]\n",
    "    img = np.random.poisson(x_image, (w, h))\n",
    "    #img = x_image\n",
    "    img = img / level\n",
    "    #print img\n",
    "    return img\n",
    "\n",
    "    \n",
    "#定义会话\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#声明输入图片数据，类别\n",
    "x = tf.placeholder('float',[None,784])\n",
    "y_ = tf.placeholder('float',[None,10])\n",
    "#输入图片数据转化\n",
    "x_image = tf.reshape(x,[-1,28,28,1])\n",
    "\n",
    "#第一层卷积层，初始化卷积核参数、偏置值，该卷积层5*5大小，一个通道，共有20个不同卷积核\n",
    "#[filter_height, filter_width, in_channels, out_channels]\n",
    "W_conv1 = weight_variable([5, 5, 1, 20])\n",
    "#进行卷积操作，并添加relu激活函数\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1))\n",
    "#进行最大池化\n",
    "h_pool1 = avg_pool_2x2(h_conv1)\n",
    "\n",
    "#同理第二层卷积层\n",
    "W_conv2 = weight_variable([5,5,20,50])\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2))\n",
    "h_pool2 = avg_pool_2x2(h_conv2)\n",
    "\n",
    "#全连接层\n",
    "#权值参数\n",
    "W_fc1 = weight_variable([4*4*50,500])\n",
    "#将卷积的产出展开\n",
    "h_pool2_flat = tf.reshape(h_pool2,[-1,4*4*50])\n",
    "#神经网络计算，并添加relu激活函数\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1))\n",
    "\n",
    "#输出层，使用softmax进行多分类\n",
    "W_fc2 = weight_variable([500,10])\n",
    "h_fc2 = tf.matmul(h_fc1, W_fc2)\n",
    "y_conv=tf.maximum(tf.nn.softmax(h_fc2),1e-30)\n",
    "\n",
    "#代价函数\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv))\n",
    "#使用Adam优化算法来调整参数\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "#测试正确率\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "#保存参数\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "#所有变量进行初始化\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#获取mnist数据\n",
    "mnist_data_set = extract_mnist.MnistDataSet('../mnist/')\n",
    "te_images,test_labels = mnist_data_set.test_data()\n",
    "test_images = input_poisson(te_images, 255.0)\n",
    "#test_images = te_images + 0.5\n",
    "\n",
    "#进行训练\n",
    "if isTrain:\n",
    "    start_time = time.time()\n",
    "    for i in xrange(10000):\n",
    "        #获取训练数据\n",
    "        xs, batch_ys = mnist_data_set.next_train_batch(batch_size)\n",
    "        batch_xs = input_poisson(xs, 50.0)\n",
    "        #print batch_xs.shape[0]\n",
    "        #每迭代100个 batch，对当前训练数据进行测试，输出当前预测准确率\n",
    "        if i%100 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={x:batch_xs, y_: batch_ys})\n",
    "            print \"step %d, training accuracy %g\"%(i, train_accuracy)\n",
    "            #计算间隔时间\n",
    "            end_time = time.time()\n",
    "            print 'time: ',(end_time - start_time)\n",
    "            start_time = end_time\n",
    "        #训练数据\n",
    "        train_step.run(feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "\n",
    "    #保存参数\n",
    "    if not tf.gfile.Exists('output/model'):\n",
    "        tf.gfile.MakeDirs('output/model')\n",
    "    save_path = saver.save(sess, \"output/model/model.ckpt\")\n",
    "    print \"Model saved in file: \", save_path\n",
    "\n",
    "    #保存网络权值\n",
    "    if not tf.gfile.Exists('output/weights'):\n",
    "        tf.gfile.MakeDirs('output/weights')\n",
    "    conv1_weights = sess.run(W_conv1)\n",
    "    conv2_weights = sess.run(W_conv2)\n",
    "    ip1_weights = sess.run(W_fc1)\n",
    "    ip2_weights = sess.run(W_fc2)\n",
    "    sio.savemat('output/weights/lenet_avg_pooling.mat', {'conv1_weights':conv1_weights, \n",
    "                                                        'conv2_weights':conv2_weights, \n",
    "                                                        'ip1_weights':ip1_weights, \n",
    "                                                        'ip2_weights':ip2_weights})\n",
    "else:\n",
    "    saver.restore(sess, \"output/model/model.ckpt\")\n",
    "# 输出整体测试数据的情况\n",
    "avg = 0\n",
    "for i in xrange(200):\n",
    "    avg += accuracy.eval(feed_dict={x: test_images[i*50:i*50+50], y_: test_labels[i*50:i*50+50]})\n",
    "avg/=200\n",
    "print \"test accuracy %g\"%avg\n",
    "\n",
    "conv1_out = sess.run(h_conv1, feed_dict={x: test_images[0,:].reshape((1,784))})\n",
    "\n",
    "# print conv1_weights[:,:,0,0]\n",
    "# tmp = conv1_out[0,:,:,0] * 255\n",
    "# print tmp.astype(int)\n",
    "\n",
    "# 关闭会话\n",
    "sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
